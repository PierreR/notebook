= Kubernetes


Control plane::
Kubernetes master cluster that includes the kube `apiserver`, `scheduler` and `controller managers`.
Usually the `etcd` cluster is part of it too (but it can be separated).

Deployments::
Collection of resources and references

DaemonSet::
Ensures that all (or some) Nodes run a copy of a Pod.

Secrets::
- Separate sensible information and flag them as such.
- Don't provide encryption (Base64 encoded).
- Set of key/value pairs

Namespaces::
Create multiple virtual clusters on the same physical clusters.
You can limit resources such as CPU per namespace.

Container Network Interface (CNI)::
ex: `flannel`

IPVS::
IP Virtual Server is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.
It can now be use instead of iptables to scale `kube-proxy` (services L4).

== Control plane

=== Pods

- kube-apiserver
- kube-scheduler
- kube-controller-manager
- kube-proxy
- kube-flannel
- https://coredns.io/[coredns]


=== Namespaces

- kube-public
- kube-system
- default


== Dashboard

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
```

.dashboard-user.yaml
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
```

```
kubectl apply -f dashboard-user.yaml
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | cut -f1 -d ' ')
```

```
kubectl proxy
```

== Services


```
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  type: ClusterIP <1>
```
<1> NodePort | LoadBalancer | ExternalName

A NodePort service is the most primitive way to get external traffic directly to your service. NodePort opens a specific port on all the nodes, and any traffic that is sent to this port is forwarded to the service.

kube-proxy::
responsible for implementing a form of virtual IP for Services (of type other than ExternalName).


== EKS



It is easy to create all required AWS resources by using the following terraform module:

https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/2.0.0

.main.tf
```
provider "aws" {
  region = "eu-central-1"
}

data "aws_region" "current" {}

data "aws_availability_zones" "available" {}

locals {
  cluster_name = "lab"
  worker_groups = [
    {
      asg_desired_capacity = 2
      instance_type        = "m4.large"
      subnets              = "${join(",", module.vpc.private_subnets)}"
    }
  ]
  tags = {
    Environment = "test"
  }
}

module "vpc" {
  source             = "terraform-aws-modules/vpc/aws"
  version            = "1.14.0"
  name               = "lab-vpc"
  cidr               = "10.0.0.0/16"
  azs                = ["${data.aws_availability_zones.available.names[0]}", "${data.aws_availability_zones.available.names[1]}", "${data.aws_availability_zones.available.names[2]}"]
  private_subnets    = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets     = ["10.0.4.0/24", "10.0.5.0/24", "10.0.6.0/24"]
  enable_nat_gateway = true
  single_nat_gateway = true
  tags               = "${merge(local.tags, map("kubernetes.io/cluster/${local.cluster_name}", "shared"))}"
}

module "eks" {
  source            = "terraform-aws-modules/eks/aws"
  cluster_name      = "${local.cluster_name}"
  subnets           = ["${module.vpc.private_subnets}"]
  tags              = "${local.tags}"
  vpc_id            = "${module.vpc.vpc_id}"
  worker_groups     = "${local.worker_groups}"
}
```

The module will create:

- A specific `IAM` role associated with the cluster
- the VPC with its subnets, gateway and route table
- the cluster (control plane)
- Security Group to allow networking traffic with EKS cluster
- AutoScaling Group to launch worker instances
- Output for the kubectl configuration

You will need to provide the necessary credentials:

.aws/credential
```
[default]
aws_access_key_id=
aws_secret_access_key=
```

You can optionally define an default region:

.aws/config
```
[default]
region=eu-north-1
```

== Probes

Liveness::
Detect if a container becomes unresponsive (need to be restarted)

Readiness::
Detect if a ready to start accepting traffic.

See https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/[Probe configuration]


== Auto-Scaling

:Todo:

== Helm

To install `helm`, install the client using the package manager of your OS.
You currently need to deploy the server into your cluster with the `helm init` command :

```
→ kubectl apply -f helm-rbac.yaml
→ helm init --service-account tiller
→ helm version
→ helm search stable/jenkins
→ helm install --name mediawiki stable/mediawiki
→ helm ls
→ helm delete mediawiki
```


== On premise

 kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml

== Swarm comparison

- attach volumes

== Resources

https://metallb.universe.tf/[metalLB]::
a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.
